# DACON_NLP_clssification
#tfgpt-2
#https://www.kaggle.com/ostamand/submission-2-bert-gpt2

from torch import cuda
device = 'cuda' if cuda.is_available() else 'cpu'
print(f'Training MNIST Model on {device}\n{"="*44}')

from google.colab import drive

drive.mount('/content/drive')

! pip install transformers
! pip install gluonnlp
! pip install mxnet

import numpy as np
import pandas as pd
%matplotlib inline
import matplotlib.pyplot as plt
import urllib.request
import transformers
import os
import tensorflow as tf
from transformers import TFGPT2Model
from tensorflow.keras.preprocessing.text import Tokenizer

from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
import gluonnlp as nlp
from gluonnlp.data import SentencepieceTokenizer
import re



filename1='/content/drive/MyDrive/news_train.csv'
filename2='/content/drive/MyDrive/news_test.csv'
filename3='/content/drive/My Drive/sample_submission.csv'
filename4='/content/drive/MyDrive/GoogleNews-vectors-negative300.bin.gz'
train_data=pd.read_csv(filename1)
test_data=pd.read_csv(filename2)
submission_data=pd.read_csv(filename3)
train_data.head()


test_data.head()

del train_data['n_id']      #content, info만 빼고 우선 지우기
del train_data['date']
del train_data['title']
del train_data['ord']
print(train_data)

del test_data['n_id']      #new_article, info만 빼고 우선 지우기
del test_data['date']
del test_data['title']
del test_data['ord']
del test_data['id']
print(test_data)

# gpt 알집 다운 여기안에 사전이랑 들어있는듯
! pip install wget
import wget
import zipfile

wget.download('https://github.com/NLP-kr/tensorflow-ml-nlp-tf2/releases/download/v1.0/gpt_ckpt.zip')

with zipfile.ZipFile('gpt_ckpt.zip') as z:
    z.extractall()

# 시각화

def plot_graphs(history, string):
    plt.plot(history.history[string])
    plt.plot(history.history['val_'+string], '')
    plt.xlabel("Epochs")
    plt.ylabel(string)
    plt.legend([string, 'val_'+string])
    plt.show()

SEED_NUM = 1234
tf.random.set_seed(SEED_NUM)
np.random.seed(SEED_NUM)

! pip install sentencepiece

#gpt2 x토크나이져 불러오기
TOKENIZER_PATH = '/content/gpt_ckpt/gpt2_kor_tokenizer.spiece'

tokenizer= SentencepieceTokenizer(TOKENIZER_PATH)
vocab = nlp.vocab.BERTVocab.from_sentencepiece(TOKENIZER_PATH,
                                               mask_token=None,
                                               sep_token='<unused0>',
                                               cls_token=None,
                                               unknown_token='<unk>',
                                               padding_token='<pad>',
                                               bos_token='<s>',
                                               eos_token='</s>')

BATCH_SIZE = 32
NUM_EPOCHS = 3
VALID_SPLIT = 0.1
SENT_MAX_LEN = 25

DATA_IN_PATH = './data_in/KOR'
DATA_OUT_PATH = "./data_out/KOR"

# 텍스트 전처리

def clean_text(sent):
    sent_clean = re.sub("[^가-힣ㄱ-ㅎㅏ-ㅣ\\s]", "", sent)
    return sent_clean

print(train_data[:5])

# train_data = train_data[:50] # for test

train_data_sents = []
train_data_labels = []

for train_sent, train_label in train_data[['content', 'info']].values:
    train_tokenized_text = vocab[tokenizer(clean_text(train_sent))]

    tokens = [vocab[vocab.bos_token]]  
    tokens += pad_sequences([train_tokenized_text], 
                            SENT_MAX_LEN, 
                            value=vocab[vocab.padding_token], 
                            padding='post').tolist()[0] 
    tokens += [vocab[vocab.eos_token]]

    train_data_sents.append(tokens)
    train_data_labels.append(train_label)

train_data_sents = np.array(train_data_sents, dtype=np.int64)
train_data_labels = np.array(train_data_labels, dtype=np.int64)


vocab_size = len(train_data_sents) + 1
print('단어 집합의 크기: {}'.format((vocab_size)))
tokenizer1 = Tokenizer()
vocabulary = tokenizer1.word_index
max_len=50

import numpy as np
import gensim

word2vec = gensim.models.KeyedVectors.load_word2vec_format('/content/drive/MyDrive/GoogleNews-vectors-negative300.bin.gz', binary = True)
embedding_matrix = np.zeros((vocab_size, 300))     # 300차원의 임베딩 매트릭스 생성
cnt = 0

for index, word in enumerate(vocabulary):          # vocabulary에 있는 토큰들을 하나씩 넘김
  if word in word2vec:                             # 넘겨받은 토큰이 word2vec에 존재하면(이미 훈련이 된 토큰이라는 의미)
    embedding_vector = word2vec[word]              # 해당 토큰에 해당하는 vector를 불러오고
    embedding_matrix[index] = embedding_vector     # 해당 위치의 embedding_matrix에 저장
  else:
    cnt += 1
    # print("word2vec에 없는 단어입니다.")
    continue

from tensorflow.keras.layers import Embedding, SpatialDropout1D, GRU, Dropout, Dense

class TFGPT2Classifier(tf.keras.Model):
    def __init__(self, dir_path, num_class):
        super(TFGPT2Classifier, self).__init__()
        
        self.gpt2 = TFGPT2Model.from_pretrained(dir_path)
        self.num_class = num_class
        self.embedding = tf.keras.layers.Embedding(vocab_size, 300, weights = [embedding_matrix], input_length = max_len)
        self.dropout = tf.keras.layers.Dropout(self.gpt2.config.summary_first_dropout)
        self.classifier = tf.keras.layers.Dense(self.num_class, 
                                                kernel_initializer=tf.keras.initializers.TruncatedNormal(stddev=self.gpt2.config.initializer_range), 
                                                name="classifier")
        
        
    def call(self, inputs):
        outputs = self.gpt2(inputs)
        gru_out = tf.keras.layers.GRU(100, activation='sigmoid')(outputs)
        dense = tf.keras.layers.Dense(256, activation="relu")(lstm_out)
        pred = tf.keras.layers.Dense(1, activation="sigmoid")(dense)
        pooled_output = outputs[0][:, -1]

        pooled_output = self.dropout(pooled_output)
        logits = self.classifier(pooled_output)

        return logits

BASE_MODEL_PATH = './gpt_ckpt'
cls_model = TFGPT2Classifier(dir_path=BASE_MODEL_PATH, num_class=2)

optimizer = tf.keras.optimizers.Adam(learning_rate=6.25e-5)
loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)
metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')
cls_model.compile(optimizer=optimizer, loss=loss, metrics=[metric])

model_name = "tf2_gpt2_news_classification"

earlystop_callback = EarlyStopping(monitor='val_accuracy', min_delta=0.0001, patience=2)

checkpoint_path = os.path.join(DATA_OUT_PATH, model_name, 'weights.h5')
checkpoint_dir = os.path.dirname(checkpoint_path)

if os.path.exists(checkpoint_dir):
    print("{} -- Folder already exists \n".format(checkpoint_dir))
else:
    os.makedirs(checkpoint_dir, exist_ok=True)
    print("{} -- Folder create complete \n".format(checkpoint_dir))
    
cp_callback = ModelCheckpoint(
    checkpoint_path, monitor='val_accuracy', verbose=1, save_best_only=True, save_weights_only=True)

history = cls_model.fit(train_data_sents, train_data_labels, 
                        epochs=NUM_EPOCHS, 
                        batch_size=BATCH_SIZE,
                        validation_split=VALID_SPLIT, 
                        callbacks=[earlystop_callback, cp_callback])

# test_data = test_data[:50] # for test

test_data_sents = []

for test_sent in test_data['content'].values:
    test_tokenized_text = vocab[tokenizer(clean_text(test_sent))]

    tokens = [vocab[vocab.bos_token]]  
    tokens += pad_sequences([test_tokenized_text], 
                            SENT_MAX_LEN, 
                            value=vocab[vocab.padding_token], 
                            padding='post').tolist()[0] 
    tokens += [vocab[vocab.eos_token]]

    test_data_sents.append(tokens)


test_data_sents = np.array(test_data_sents, dtype=np.int64)


cls_model.load_weights(checkpoint_path)

pred_test = [i for i,k in cls_model.predict(test_data_sents)]
pred_test = np.array(pred_test)
pred_test = pred_test.reshape(-1, 1)
submission_data.loc[:,'info'] = np.where(pred_test> 0.5, 0,1).reshape(-1)
submission_data.loc[:,["id","info"]].to_csv("submission_data(Mecab_gpt2_)1230.csv", index = False)
submission_data



























